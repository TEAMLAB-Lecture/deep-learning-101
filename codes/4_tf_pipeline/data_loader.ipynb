{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T02:12:02.405668Z",
     "start_time": "2017-08-30T02:12:02.403109Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T02:12:02.711699Z",
     "start_time": "2017-08-30T02:12:02.708489Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n02085620-Chihuahua',\n",
       " 'n02085782-Japanese_spaniel',\n",
       " 'n02085936-Maltese_dog',\n",
       " 'n02086079-Pekinese',\n",
       " 'n02086240-Shih-Tzu']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_DIR = \"Images\"\n",
    "ANNOTATION_DIR = \"Annotation\"\n",
    "\n",
    "bleeds = os.listdir(ANNOTATION_DIR)\n",
    "\n",
    "bleeds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T02:12:02.928589Z",
     "start_time": "2017-08-30T02:12:02.926036Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_set = {}\n",
    "annotation_set = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T02:12:03.646004Z",
     "start_time": "2017-08-30T02:12:03.257740Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for bleed in bleeds:\n",
    "    image_dir = os.path.join(IMAGE_DIR, bleed)\n",
    "    annotation_dir = os.path.join(ANNOTATION_DIR, bleed)\n",
    "    image_set[bleed] = os.listdir(image_dir)\n",
    "    annotation_set[bleed] = os.listdir(annotation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T02:12:03.665491Z",
     "start_time": "2017-08-30T02:12:03.647360Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_data = []\n",
    "for bleed in image_set.keys():\n",
    "    total_data.extend([ [filename,bleed] for filename in image_set[bleed]])\n",
    "total_data = np.array(total_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T02:12:04.457043Z",
     "start_time": "2017-08-30T02:12:03.980012Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     total_data[:,0], total_data[:,1], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T02:12:04.621155Z",
     "start_time": "2017-08-30T02:12:04.614135Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "x_data = X_train\n",
    "y_data = y_train\n",
    "current_index = 0\n",
    "\n",
    "\n",
    "record_location = \"./output/images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T07:46:25.219656Z",
     "start_time": "2017-08-12T07:46:14.701341Z"
    }
   },
   "outputs": [],
   "source": [
    "writer = None\n",
    "current_index = 0\n",
    "\n",
    "\n",
    "for images_filename,breed in zip(x_data,y_data):\n",
    "\n",
    "    if current_index % 100 == 0:\n",
    "        \n",
    "        if writer:\n",
    "            writer.close()\n",
    "        record_filename = \"{record_location}-{current_index}.tfrecords\".format(\n",
    "            record_location=record_location, current_index=current_index\n",
    "        )\n",
    "\n",
    "        writer = tf.python_io.TFRecordWriter(record_filename)\n",
    "        file_full_path = os.path.join(IMAGE_DIR, breed,  images_filename)\n",
    "                \n",
    "        image_file = tf.read_file(file_full_path)\n",
    "        try:\n",
    "            image = tf.image.decode_jpeg(image_file)\n",
    "        except:\n",
    "            print(\"Error : \", images_filename)\n",
    "            continue\n",
    "\n",
    "\n",
    "        grayscale_image = tf.image.rgb_to_grayscale(image)\n",
    "        resized_image = tf.image.resize_images(grayscale_image, [250, 151])\n",
    "        \n",
    "        image_bytes = sess.run(tf.cast(resized_image, tf.uint8)).tobytes()\n",
    "        image_label = breed.encode(\"utf-8\")\n",
    "        \n",
    "        example = tf.train.Example(features = tf.train.Features(\n",
    "                                    feature={'label': \n",
    "                                              tf.train.Feature(bytes_list=tf.train.BytesList(\n",
    "                                                  value=[image_label])), \n",
    "                                              \"images\":\n",
    "                                              tf.train.Feature(bytes_list=tf.train.BytesList(\n",
    "                                                  value=[image_bytes]))\n",
    "                                             }\n",
    "                                  ))\n",
    "        writer.write(example.SerializeToString())\n",
    "    current_index += 1\n",
    "    writer.close()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T07:46:55.273564Z",
     "start_time": "2017-08-12T07:46:55.148402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 238\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filenmae_queue = tf.train.string_input_producer(tf.train.match_filenames_once(record_location+\"*.tfrecords\"))\n",
    "\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized = reader.read(filenmae_queue)\n",
    "\n",
    "features = tf.parse_single_example(\n",
    "    serialized,\n",
    "    features={\n",
    "        \"label\" : tf.FixedLenFeature([], tf.string),\n",
    "        \"image\" : tf.FixedLenFeature([], tf.string),\n",
    "    }\n",
    ")\n",
    "\n",
    "record_image = tf.decode_raw(features['image'], tf.uint8)\n",
    "image = tf.reshape(record_image, [250,151,1])\n",
    "label = tf.cast(features['label'], tf.string)\n",
    "\n",
    "min_after_dequeue = 10\n",
    "batch_size = 512\n",
    "capacity = min_after_dequeue + 3 * batch_size\n",
    "\n",
    "image_batch, label_batch = tf.train.shuffle_batch([image, label], batch_size=batch_size, capacity=capacity, min_after_dequeue=min_after_dequeue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T08:33:31.432308Z",
     "start_time": "2017-08-12T08:33:31.394052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\Anaconda3\\envs\\dl\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool:0' shape=(512, 125, 76, 32) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_image_batch = tf.image.convert_image_dtype(image_batch, tf.float32)\n",
    "\n",
    "conv2d_layer_one = tf.contrib.layers.convolution2d(float_image_batch, \n",
    "                                                   num_outputs=32,\n",
    "                                                   activation_fn=tf.nn.relu,\n",
    "                                                   kernel_size=(5,5),\n",
    "                                                   stride=(1,1),\n",
    "                                                   trainable=True)\n",
    "pool_layer_one = tf.nn.max_pool(conv2d_layer_one, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "pool_layer_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T08:36:01.275532Z",
     "start_time": "2017-08-12T08:36:01.246857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Conv_1/Relu:0' shape=(512, 125, 76, 64) dtype=float32>,\n",
       " <tf.Tensor 'MaxPool_1:0' shape=(512, 63, 38, 64) dtype=float32>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d_layer_two = tf.contrib.layers.convolution2d(pool_layer_one, \n",
    "                                                   num_outputs=64,\n",
    "                                                   activation_fn=tf.nn.relu,\n",
    "                                                   kernel_size=(5,5),\n",
    "                                                   stride=(1,1),\n",
    "                                                   trainable=True)\n",
    "\n",
    "pool_layer_two = tf.nn.max_pool(conv2d_layer_two, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "conv2d_layer_two, pool_layer_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T08:39:24.182435Z",
     "start_time": "2017-08-12T08:39:24.174086Z"
    }
   },
   "outputs": [],
   "source": [
    "flattened_layer_two = tf.reshape(\n",
    "    pool_layer_two,\n",
    "    [\n",
    "        batch_size, -1\n",
    "    ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T08:40:13.424090Z",
     "start_time": "2017-08-12T08:40:13.402710Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "hidden_layer_three = tf.contrib.layers.fully_connected(\n",
    "    flattened_layer_two, num_outputs = batch_size\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
