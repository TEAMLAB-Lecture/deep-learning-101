# deep_learning_101

어떤질문들에 대답해야 할까?
- 딥러닝이 왜 좋아졌어요? -> 발전한 계기
- 딥러닝이 어떻게 발전했어요? -> 역사
- 모델을 어떻게 만들까요?
- 프레임워크간의 차이는 어떻게 되나요?
- 이미지 처리는 어떻게 하면 되나요?
- 딥러닝으로 코딩한다는 의미? 조건절 vs. 확률 프로그래밍
- 학습이 왜 잘 안되요
  - gradient vanishing
  - overfitting
  - SLOW
- Backpropagation의 기본 원리
  - SoftMax
  - Sigmoid

- LOSS functions
- Learning rate
- Optimizer
  - Stochastic GD
  - mini-batch
  - epoch의 의미?
  - visualization
  - Momentum
  - Adagrad
  - NAG
  - Nadam
  - Adam
  - RMSProp
  - AdaDelta
  - 비쥬얼라이제인션이랑 Numpy 코드를제시해주자
- Activation functions
  - ReLU
  - Sigmoid
  - Tahn
  - 근데 이게 어떻게 다른거에요? 왜 Sigmoid가 아닌 Relu를 쓰죠?
- Overfitting 극복
  - Dropout
- ImageNet 이야기
  - https://image.slidesharecdn.com/random-170910154045/95/-64-638.jpg?cb=1505089848


- CNN
  - CNN이 왜 나왔는지에 대해서 얘기해줘야 한다.
  - Stide, Padding, MaxPooling
   https://image.slidesharecdn.com/random-170910154045/95/-83-638.jpg?cb=1505089848
  - AlexNet
  - VGG
  - GoogleNet
  - ResNet
  - InceptionV3
- RNN
  - Valila RNN
  - 왜 RNN이 나왔을까?
  - LSTM 구조
  - GRU
  - Bidirectional-LSTM
  - Attention 모델
- 어떤 분석을 하고 싶어?
- 이미지 처리 - 분류, object deection
  - Data augumentation
  - 데이터 구조가 어떻게 생겼는지 얘기해줘야 함
  -
- 텍스트 처리 - 분류,
- Log 데이터 분석
- Structed data 분석하기
