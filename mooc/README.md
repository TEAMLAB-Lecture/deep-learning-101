# Deep Learning 101 with Tensorflow 2.0

## ch1. 딥러닝 소개하기
## ch2. Linear Regression and Gradient Descent
## ch3. Logistic Regression and Softxmax Function
## ch4. Neural Network
    - 어떤 데이터로 할까? MNIST, Fashion-MNIST
## ch5. Building Blocks for Neural Network 
    - Activation Functions
    - Gradient Descent Algorithms
    - Hyper parameters
    - 데이터 셋 만들기
## ch6. How to improve the performance of your model
    - 개고양이 실험 , 또는 
    - 실험하는 방법
    - 성능을 측정하는 방법
        - Accuracy
        - Precision
        - Recall 
        - MAP
        - Multi-labeling problems
    - Data augumentation
    - Dropout families 
    - Batch Normalization Familes

## ch7. Convolution Neurla Network
    - CNN basic concepts
    - AlexNet & VGG Network
    - GoogLeNet & Inception
    - ResNet
## ch8. Advanced CNN
    -
    - Transfer learning
    - TFHub
    - Freezing

## ch9. CNN and Applications  
    - Neural Transfer
    - Object Detection
    - Image segmentation
## ch10. Embedings and Text Processing
    - Text-Cnn Paper Review
    - Embedding concepts
    - Word2Vec
    - Korean Word2Vec
    - GloVe and FastText
    - Doc2Vec and Document embeddings

## ch11. Recurrent Neural Network (RNN)
    - RNN의 기본 개념
    - RNN 수식
    - RNN의 구현 Zero to All
    - RNN architecture
    - RNN 구현 Keras
    - LSTM 
    - GRU

## ch12. RNN and applications
    - Simple Text Generation
    - Image Captioning
    - Visual Question Ansewring
    - LSTM for time series

## ch13. Machine Translation and Transformer
    - Attention Mechanism
    - Attention is all you need: Transformer  
    - Elmo : Context embedding
    - Open AI GPT
    - BERT: Pre-training of Deep Bidirectional Transformers

## ch14. Unsupervised Learning - GAN
## ch15. Unsupervised Learning - VAE
## ch16. New Architectures for applications
    - CapSule Network
    - 
    - DeepWalk
    - GCN
    - Diff2Vec













